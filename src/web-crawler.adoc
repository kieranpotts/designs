= Web crawler system design

== Requirements

=== Functional requirements

* The system should be able to fetch URLs from the web efficiently.

* The crawler should be able to handle different content types: text, images, and multimedia.

* The system should be able to prioritize URLs based on specific criteria, such as importance or freshness.

* Crawled data should be stored as efficiently as possible â€“ there is expected to be a lot of it.

=== Non-functional requirements

* Scalability: the system should be able to scale to handle millions, even billions, of pages.

* Latency: minimize the time it takes to fetch and process web pages.

* Throughput: optimize the number of pages crawled per unit of time.
